---
layout: ../layouts/BlogPost.astro
title: State of AI Report 2024
slug: state-of-ai-report-2024
description: How would AI become in 2024 and next?
tags:
  - technical
added: 2024-10-11T06:06:09.828Z
---

🥇 OpenAI's Dominance Challenged
OpenAI's dominance in AI research has been challenged
Advanced labs like Claude 3.5 Sonnet, Gemini 1.5, and Grok 2 have narrowed the performance gap with GPT-4
Model performance is converging
💡 OpenAI's Reasoning Improvement
Significant improvement in LLM reasoning capabilities
Shifted computational resources from pre-training and post-training to inference stage
"Chain-of-thought" technique uses reinforcement learning
Unlocked ability to solve multi-step math, science, and programming problems
🔓 Open-Source Models Catching Up
Open-source models narrowing the gap with proprietary advanced models
Meta's Llama 3, released in three versions from April to September 2024
Competitive with GPT-4 and Claude 3.5 Sonnet in reasoning, math, multilingual, and long-context tasks
⚠️ Data Contamination Concerns
Concerns about "dataset contamination"
Test or validation data leaking into training sets
Significant performance drops observed when models tested on new datasets reflecting established benchmark styles and complexity
🛠️ Benchmark Improvement Efforts
Researchers working to address issues in widely used benchmarks
OpenAI warned that SWE-bench, evaluating real-world software problem-solving, underestimated models' automated software engineering capabilities
🤔 "Vibe" Evaluation Concerns
Concerns about community "vibe" evaluation method for preferred models
LMYS Chatbot Arena rankings allow users to interact with randomly selected chatbots and provide community ratings
Controversial results: GPT-4 and GPT-4 Mini receiving same score, GPT-4 Mini even surpassing Claude Sonnet 3.5
🧠 Neuro-Symbolic Systems Resurgence
Addressing weaknesses in reasoning and training data
Google DeepMind/NYU team created AlphaGeometry, a symbolic reasoning tool for geometry problems
✂️ Model Pruning Without Performance Loss
Research shows models perform well when deeper layers are skillfully pruned
These layers designed to process complex, abstract, or task-specific information
🧪 Distillation Models Gaining Popularity
Using large models to refine and synthesize training data for smaller models
Google applied this method:
Distilled Gemini 1.5 Flash from Gemini 1.5 Pro
Gemma 2.9B distilled from Gemma 2.27B
Gemma 2B from a larger unreleased model
📱 Mobile-Optimized Models Competing
Models built for mobile devices now competing with larger models
Major tech companies deploying AI at scale for end-users
Using high-performance but small LLM and multimodal models that can run on smartphones
🤏 Quantization for On-Device AI
Quantization yields good results for early on-device AI deployment
Reduces LLM memory requirements while maintaining acceptable AI performance
🧑‍💼 On-Device Personalization Progress
Representation fine-tuning (ReFT) adjusts model's internal workings at inference time
ReFT requires 15-65 times fewer parameters than weight-based fine-tuning methods
🔗 Hybrid Models Gaining Attention
Combining attention mechanisms with other mechanisms
Maintaining or improving accuracy while reducing computational costs and memory usage
🌱 Synthetic Data Adoption and Concerns
Wider adoption of synthetic data
Concerns about "model collapse" when models are trained on too much synthetic data
🕸️ Large-Scale Web Data Publication
Hugging Face built a 15T token dataset for LLM pre-training using 96 CommonCrawl snapshots
Resulting LLMs outperform models trained on other open pre-training datasets
🔍 Retrieval and Embeddings Focus
Increased interest due to importance of RAG (Retrieval-Augmented Generation)
🎯 Context Importance in RAG
Context proven crucial for RAG performance
RAG evaluation remains unresolved
⚡ Power Shortage and Solutions
Advanced labs facing power shortages
Google DeepMind proposed optimization algorithm for training across loosely connected device "clusters"
🏋️‍♀️ Reducing Training Computation Requirements
Researchers exploring methods to reduce training computational demands
Google DeepMind developed JEST, flexibly selecting training examples based on learning potential
🇨🇳 Chinese Labs' Contributions
Significant contributions to AI research despite U.S. sanctions
🎨 Advanced Diffusion Models for Image Generation
Improvements in quality and efficiency
🧬 AlphaFold 3 Advancements
Can now model interactions between small molecule drugs, DNA, RNA, antibodies, and protein targets
Decision not to release AlphaFold 3 source code controversial
🦠 AlphaProteo Release
DeepMind's generative model for designing sub-nanomolar protein binders
3 to 300 times better affinity than previous methods
🔄 Equivariance Challenged
Core idea in AI research being challenged
Recent research shows non-equivariant models can perform better in some cases
🔬 Foundation Models in Various Scientific Fields
Development across biology, inorganic materials, brain activity, and atmospheric science
🏆 ARC Prize
New competition aiming to refocus AI industry on path to AGI
🧩 LLM Challenges in Planning and Simulation
Researchers exploring new methods to improve planning and reasoning capabilities
🔎 Program Search Algorithms in Mathematics
Opening new discoveries in mathematics
📈 RL Driving VLM Performance Improvements
🤖 Large-Scale RL Agent Training
Researchers exploring training methods using foundation models
🔬 Automating Research with Foundation Models
🤝 Ensemble Methods in Code Generation
Driving strong performance improvements
🚗 Multi-Modal Approaches in Self-Driving Cars
🎥 Segment Anything Extended to Video
Meta's impressive image segmentation model expanded to video
🤖 Robot Research Resurgence
⚙️ Diffusion Models in Robot Policy and Action Generation
🔄 Expanding Real-World Robot Data
🍎 Apple Vision Pro as Essential Robot Research Tool
⚕️ Fine-Tuned Multimodal Models in Medical AI
Achieving state-of-the-art results
💊 Synthetic Data Generation in Medicine
🏢 AI in Enterprise Automation Tools
🌍 Global AI Research Power Balance
Generally unchanged, but academia contributing more than companies
🟩 NVIDIA's Continued Dominance
(Note: The last point seems to be cut off in the original text)
