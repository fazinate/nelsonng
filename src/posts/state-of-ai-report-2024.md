---
layout: ../layouts/BlogPost.astro
title: State of AI Report 2024
slug: state-of-ai-report-2024
description: How would AI become in 2024 and next?
tags:
  - technical
added: 2024-10-11T06:06:09.828Z
---

ğŸ¥‡ OpenAI's Dominance Challenged
OpenAI's dominance in AI research has been challenged
Advanced labs like Claude 3.5 Sonnet, Gemini 1.5, and Grok 2 have narrowed the performance gap with GPT-4
Model performance is converging
ğŸ’¡ OpenAI's Reasoning Improvement
Significant improvement in LLM reasoning capabilities
Shifted computational resources from pre-training and post-training to inference stage
"Chain-of-thought" technique uses reinforcement learning
Unlocked ability to solve multi-step math, science, and programming problems
ğŸ”“ Open-Source Models Catching Up
Open-source models narrowing the gap with proprietary advanced models
Meta's Llama 3, released in three versions from April to September 2024
Competitive with GPT-4 and Claude 3.5 Sonnet in reasoning, math, multilingual, and long-context tasks
âš ï¸ Data Contamination Concerns
Concerns about "dataset contamination"
Test or validation data leaking into training sets
Significant performance drops observed when models tested on new datasets reflecting established benchmark styles and complexity
ğŸ› ï¸ Benchmark Improvement Efforts
Researchers working to address issues in widely used benchmarks
OpenAI warned that SWE-bench, evaluating real-world software problem-solving, underestimated models' automated software engineering capabilities
ğŸ¤” "Vibe" Evaluation Concerns
Concerns about community "vibe" evaluation method for preferred models
LMYS Chatbot Arena rankings allow users to interact with randomly selected chatbots and provide community ratings
Controversial results: GPT-4 and GPT-4 Mini receiving same score, GPT-4 Mini even surpassing Claude Sonnet 3.5
ğŸ§  Neuro-Symbolic Systems Resurgence
Addressing weaknesses in reasoning and training data
Google DeepMind/NYU team created AlphaGeometry, a symbolic reasoning tool for geometry problems
âœ‚ï¸ Model Pruning Without Performance Loss
Research shows models perform well when deeper layers are skillfully pruned
These layers designed to process complex, abstract, or task-specific information
ğŸ§ª Distillation Models Gaining Popularity
Using large models to refine and synthesize training data for smaller models
Google applied this method:
Distilled Gemini 1.5 Flash from Gemini 1.5 Pro
Gemma 2.9B distilled from Gemma 2.27B
Gemma 2B from a larger unreleased model
ğŸ“± Mobile-Optimized Models Competing
Models built for mobile devices now competing with larger models
Major tech companies deploying AI at scale for end-users
Using high-performance but small LLM and multimodal models that can run on smartphones
ğŸ¤ Quantization for On-Device AI
Quantization yields good results for early on-device AI deployment
Reduces LLM memory requirements while maintaining acceptable AI performance
ğŸ§‘â€ğŸ’¼ On-Device Personalization Progress
Representation fine-tuning (ReFT) adjusts model's internal workings at inference time
ReFT requires 15-65 times fewer parameters than weight-based fine-tuning methods
ğŸ”— Hybrid Models Gaining Attention
Combining attention mechanisms with other mechanisms
Maintaining or improving accuracy while reducing computational costs and memory usage
ğŸŒ± Synthetic Data Adoption and Concerns
Wider adoption of synthetic data
Concerns about "model collapse" when models are trained on too much synthetic data
ğŸ•¸ï¸ Large-Scale Web Data Publication
Hugging Face built a 15T token dataset for LLM pre-training using 96 CommonCrawl snapshots
Resulting LLMs outperform models trained on other open pre-training datasets
ğŸ” Retrieval and Embeddings Focus
Increased interest due to importance of RAG (Retrieval-Augmented Generation)
ğŸ¯ Context Importance in RAG
Context proven crucial for RAG performance
RAG evaluation remains unresolved
âš¡ Power Shortage and Solutions
Advanced labs facing power shortages
Google DeepMind proposed optimization algorithm for training across loosely connected device "clusters"
ğŸ‹ï¸â€â™€ï¸ Reducing Training Computation Requirements
Researchers exploring methods to reduce training computational demands
Google DeepMind developed JEST, flexibly selecting training examples based on learning potential
ğŸ‡¨ğŸ‡³ Chinese Labs' Contributions
Significant contributions to AI research despite U.S. sanctions
ğŸ¨ Advanced Diffusion Models for Image Generation
Improvements in quality and efficiency
ğŸ§¬ AlphaFold 3 Advancements
Can now model interactions between small molecule drugs, DNA, RNA, antibodies, and protein targets
Decision not to release AlphaFold 3 source code controversial
ğŸ¦  AlphaProteo Release
DeepMind's generative model for designing sub-nanomolar protein binders
3 to 300 times better affinity than previous methods
ğŸ”„ Equivariance Challenged
Core idea in AI research being challenged
Recent research shows non-equivariant models can perform better in some cases
ğŸ”¬ Foundation Models in Various Scientific Fields
Development across biology, inorganic materials, brain activity, and atmospheric science
ğŸ† ARC Prize
New competition aiming to refocus AI industry on path to AGI
ğŸ§© LLM Challenges in Planning and Simulation
Researchers exploring new methods to improve planning and reasoning capabilities
ğŸ” Program Search Algorithms in Mathematics
Opening new discoveries in mathematics
ğŸ“ˆ RL Driving VLM Performance Improvements
ğŸ¤– Large-Scale RL Agent Training
Researchers exploring training methods using foundation models
ğŸ”¬ Automating Research with Foundation Models
ğŸ¤ Ensemble Methods in Code Generation
Driving strong performance improvements
ğŸš— Multi-Modal Approaches in Self-Driving Cars
ğŸ¥ Segment Anything Extended to Video
Meta's impressive image segmentation model expanded to video
ğŸ¤– Robot Research Resurgence
âš™ï¸ Diffusion Models in Robot Policy and Action Generation
ğŸ”„ Expanding Real-World Robot Data
ğŸ Apple Vision Pro as Essential Robot Research Tool
âš•ï¸ Fine-Tuned Multimodal Models in Medical AI
Achieving state-of-the-art results
ğŸ’Š Synthetic Data Generation in Medicine
ğŸ¢ AI in Enterprise Automation Tools
ğŸŒ Global AI Research Power Balance
Generally unchanged, but academia contributing more than companies
ğŸŸ© NVIDIA's Continued Dominance
(Note: The last point seems to be cut off in the original text)
